# main.py - Fixed v·ªõi lifespan v√† error handling
import os
import sys
import json
import asyncio
import logging
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn

# C·∫•u h√¨nh logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Th√™m th∆∞ m·ª•c g·ªëc v√†o sys.path
sys.path.append(os.path.abspath('.'))

from models.gemma_model import GemmaModel
from models.rag_pipeline import RAGPipeline

# Global variables
gemma_model = None
rag_pipeline = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Kh·ªüi t·∫°o v√† d·ªçn d·∫πp resources v·ªõi proper error handling"""
    global gemma_model, rag_pipeline
    
    print("===== Application Startup =====")
    logger.info("üöÄ Kh·ªüi t·∫°o Gemma model v√† RAG pipeline...")
    
    try:
        # T·∫°o th∆∞ m·ª•c cache n·∫øu c·∫ßn
        cache_dir = "/tmp/hf_cache"
        os.makedirs(cache_dir, exist_ok=True)
        
        gemma_model = GemmaModel()
        rag_pipeline = RAGPipeline(gemma_model)
        logger.info("‚úÖ Kh·ªüi t·∫°o th√†nh c√¥ng!")
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói kh·ªüi t·∫°o: {e}")
        # Kh√¥ng raise exception ƒë·ªÉ app v·∫´n c√≥ th·ªÉ start
        logger.warning("‚ö†Ô∏è App s·∫Ω ch·∫°y m√† kh√¥ng c√≥ model")
    
    yield
    
    # Cleanup (if needed)
    logger.info("üîÑ ƒêang d·ªçn d·∫πp resources...")

# Kh·ªüi t·∫°o FastAPI app v·ªõi lifespan
app = FastAPI(
    title="RAG Product Chatbot API",
    description="API streaming cho chatbot RAG s·∫£n ph·∫©m",
    version="1.0.0",
    lifespan=lifespan  # S·ª≠ d·ª•ng lifespan thay v√¨ on_event
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Trong production n√™n gi·ªõi h·∫°n
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Pydantic models
class ChatRequest(BaseModel):
    query: str
    max_tokens: int = 512

class ChatResponse(BaseModel):
    response: str

@app.get("/")
async def root():
    """Health check endpoint"""
    return {
        "message": "RAG Product Chatbot API", 
        "status": "ready" if rag_pipeline else "model_loading",
        "version": "1.0.0",
        "model_loaded": gemma_model is not None,
        "endpoints": {
            "health": "/health",
            "chat": "/api/chat", 
            "stream": "/api/chat/stream"
        }
    }

@app.get("/health")
async def health_check():
    """Chi ti·∫øt health check"""
    return {
        "status": "healthy" if rag_pipeline else "initializing",
        "model_loaded": gemma_model is not None,
        "rag_ready": rag_pipeline is not None,
        "cache_dir": os.environ.get("HF_HOME", "/tmp/hf_cache"),
        "timestamp": asyncio.get_event_loop().time()
    }

@app.post("/api/chat", response_model=ChatResponse)
async def chat_non_streaming(request: ChatRequest):
    """API kh√¥ng streaming"""
    if not rag_pipeline:
        return ChatResponse(response="‚ö†Ô∏è Model ƒëang ƒë∆∞·ª£c kh·ªüi t·∫°o, vui l√≤ng th·ª≠ l·∫°i sau.")
    
    try:
        response = rag_pipeline.answer_question(request.query, request.max_tokens)
        return ChatResponse(response=response)
    except Exception as e:
        logger.error(f"Error in chat: {e}")
        return ChatResponse(response=f"‚ùå L·ªói x·ª≠ l√Ω: {str(e)}")

@app.post("/api/chat/stream")
async def chat_streaming(request: ChatRequest):
    """API streaming"""
    if not rag_pipeline:
        async def error_stream():
            yield f"data: {json.dumps({'type': 'error', 'content': '‚ö†Ô∏è Model ƒëang ƒë∆∞·ª£c kh·ªüi t·∫°o, vui l√≤ng th·ª≠ l·∫°i sau.'})}\n\n"
        
        return StreamingResponse(
            error_stream(),
            media_type="text/event-stream"
        )

    async def generate_stream():
        try:
            yield f"data: {json.dumps({'type': 'start', 'content': 'üöÄ ƒêang x·ª≠ l√Ω c√¢u h·ªèi...'})}\n\n"
            
            for token in rag_pipeline.answer_question_stream(request.query, request.max_tokens):
                yield f"data: {json.dumps({'type': 'token', 'content': token})}\n\n"
                await asyncio.sleep(0.01)
            
            yield f"data: {json.dumps({'type': 'end', 'content': '‚úÖ Ho√†n t·∫•t!'})}\n\n"
            
        except Exception as e:
            logger.error(f"Error in streaming: {e}")
            yield f"data: {json.dumps({'type': 'error', 'content': f'‚ùå L·ªói: {str(e)}'})}\n\n"

    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        }
    )

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8000))
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=port,
        reload=False
    )