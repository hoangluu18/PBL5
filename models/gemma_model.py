# #
# #
# # import torch
# # from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
# #
# #
# # class GemmaModel:
# #     def __init__(self):
# #
# #         # C·∫•u h√¨nh l∆∞·ª£ng t·ª≠ h√≥a ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ
# #         quantization_config = BitsAndBytesConfig(
# #             load_in_8bit=True  # S·ª≠ d·ª•ng l∆∞·ª£ng t·ª≠ h√≥a 8bit ƒë·ªÉ gi·∫£m dung l∆∞·ª£ng b·ªô nh·ªõ
# #         )
# #
# #         # Kh·ªüi t·∫°o tokenizer
# #         self.model_id = "google/gemma-3-1b-it"
# #         self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
# #
# #         # T·∫£i m√¥ h√¨nh
# #         print("ƒêang t·∫£i m√¥ h√¨nh Gemma 3...")
# #         self.model = AutoModelForCausalLM.from_pretrained(
# #             self.model_id,
# #             quantization_config=quantization_config,
# #             device_map="auto"  # T·ª± ƒë·ªông ph√¢n b·ªï m√¥ h√¨nh v√†o GPU ho·∫∑c CPU
# #         ).eval()
# #         print("ƒê√£ t·∫£i xong m√¥ h√¨nh Gemma 3!")
# #
# #     def generate_response(self, query, context=None, max_new_tokens=512):
# #         """
# #         T·∫°o c√¢u tr·∫£ l·ªùi cho c√¢u h·ªèi d·ª±a tr√™n ng·ªØ c·∫£nh (n·∫øu c√≥)
# #
# #         Args:
# #             query (str): C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
# #             context (str, optional): Th√¥ng tin ng·ªØ c·∫£nh t·ª´ RAG
# #             max_new_tokens (int): S·ªë l∆∞·ª£ng token t·ªëi ƒëa cho c√¢u tr·∫£ l·ªùi
# #
# #         Returns:
# #             str: C√¢u tr·∫£ l·ªùi c·ªßa m√¥ h√¨nh
# #         """
# #         # X√¢y d·ª±ng prompt d·ª±a tr√™n ng·ªØ c·∫£nh RAG
# #         if context:
# #             system_prompt = "B·∫°n l√† tr·ª£ l√Ω AI tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ s·∫£n ph·∫©m d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p. Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin c√≥ trong d·ªØ li·ªáu ƒë√£ cho. N·∫øu kh√¥ng c√≥ th√¥ng tin, h√£y n√≥i r·∫±ng b·∫°n kh√¥ng c√≥ ƒë·ªß th√¥ng tin."
# #             prompt = f"Th√¥ng tin s·∫£n ph·∫©m:\n{context}\n\nC√¢u h·ªèi: {query}"
# #         else:
# #             system_prompt = "B·∫°n l√† tr·ª£ l√Ω AI tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ s·∫£n ph·∫©m. N·∫øu kh√¥ng c√≥ ƒë·ªß th√¥ng tin, h√£y tr·∫£ l·ªùi r·∫±ng b·∫°n kh√¥ng c√≥ th√¥ng tin v·ªÅ s·∫£n ph·∫©m ƒë√≥."
# #             prompt = query
# #
# #         # T·∫°o c·∫•u tr√∫c tin nh·∫Øn theo ƒë·ªãnh d·∫°ng chat
# #         messages = [
# #             [
# #                 {
# #                     "role": "system",
# #                     "content": [{"type": "text", "text": system_prompt}]
# #                 },
# #                 {
# #                     "role": "user",
# #                     "content": [{"type": "text", "text": prompt}]
# #                 }
# #             ]
# #         ]
# #
# #         # √Åp d·ª•ng chat template v√† tokenize
# #         inputs = self.tokenizer.apply_chat_template(
# #             messages,
# #             add_generation_prompt=True,
# #             tokenize=True,
# #             return_dict=True,
# #             return_tensors="pt"
# #         ).to(self.model.device)
# #
# #         # Sinh c√¢u tr·∫£ l·ªùi
# #         with torch.inference_mode():
# #             outputs = self.model.generate(
# #                 **inputs,
# #                 max_new_tokens=max_new_tokens,
# #                 temperature=0.7,
# #                 top_p=0.9,
# #                 do_sample=True
# #             )
# #
# #         # Gi·∫£i m√£ v√† tr·∫£ v·ªÅ k·∫øt qu·∫£
# #         decoded_output = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]
# #
# #         # X·ª≠ l√Ω k·∫øt qu·∫£ ƒë·ªÉ ch·ªâ l·∫•y ph·∫ßn ph·∫£n h·ªìi c·ªßa m√¥ h√¨nh
# #         assistant_prefix = "assistant"
# #         if assistant_prefix in decoded_output.lower():
# #             response = decoded_output.lower().split(assistant_prefix, 1)[1].strip()
# #             if response.startswith(": "):
# #                 response = response[2:]
# #             return response
# #
# #         return decoded_output
#
#
# import torch
# from transformers import AutoTokenizer, AutoModelForCausalLM
#
#
# class GemmaModel:
#     def __init__(self):
#         # Kh·ªüi t·∫°o tokenizer
#         self.model_id = "google/gemma-3-1b-it"
#         #self.model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
#         #self.model_id = "microsoft/phi-3-mini-4k-instruct-q4"
#         self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
#
#         # T·∫£i m√¥ h√¨nh kh√¥ng s·ª≠ d·ª•ng l∆∞·ª£ng t·ª≠ h√≥a
#         print("ƒêang t·∫£i m√¥ h√¨nh Gemma 3 v√†o CPU...")
#         self.model = AutoModelForCausalLM.from_pretrained(
#             self.model_id,
#             device_map="cpu",  # Ch·ªâ ƒë·ªãnh r√µ s·ª≠ d·ª•ng CPU
#             torch_dtype=torch.float32  # S·ª≠ d·ª•ng ƒë·ªô ch√≠nh x√°c ƒë·∫ßy ƒë·ªß
#         ).eval()
#         print("ƒê√£ t·∫£i xong m√¥ h√¨nh Gemma 3!")
#
#
#     def generate_response(self, query, context=None, max_new_tokens=512):
#         """
#         T·∫°o c√¢u tr·∫£ l·ªùi cho c√¢u h·ªèi d·ª±a tr√™n ng·ªØ c·∫£nh (n·∫øu c√≥)
#         """
#         # X√¢y d·ª±ng prompt d·ª±a tr√™n ng·ªØ c·∫£nh RAG
#         if context:
#             system_prompt = """
# Tr·∫£ l·ªùi v·ªÅ s·∫£n ph·∫©m theo quy t·∫Øc:
# 1. Li·ªát k√™ T·∫§T C·∫¢ s·∫£n ph·∫©m ƒë∆∞·ª£c cung c·∫•p
# 2. ƒê·∫ßu ti√™n vi·∫øt: "T√¥i t√¨m th·∫•y X s·∫£n ph·∫©m li√™n quan:"
# 3. Ti·∫øp theo l√† ƒë∆∞·ªùng link d·∫´n t·ªõi s·∫£n ph·∫©m ƒë√≥: Ch√∫ √Ω THAY "alias" b·∫±ng gi√° tr·ªã TH·∫¨T t·ª´ s·∫£n ph·∫©m
#    V√ç D·ª§ C·ª§ TH·ªÇ: [Gi√†y Adidas Ultraboost 21](http://localhost:5173/p/giay-adidas-ultraboost-21)
# 4. M·ªói s·∫£n ph·∫©m c·∫ßn: link, gi√°, th∆∞∆°ng hi·ªáu, tr·∫°ng th√°i
# 5. D√πng g·∫°ch ƒë·∫ßu d√≤ng n·∫øu c√≥ nhi·ªÅu s·∫£n ph·∫©m
# Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, ng·∫Øn g·ªçn.
#             """
#             prompt = f"Th√¥ng tin s·∫£n ph·∫©m:\n{context}\n\nC√¢u h·ªèi: {query}"
#         else:
#             system_prompt = "B·∫°n l√† tr·ª£ l√Ω AI tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ s·∫£n ph·∫©m. Hi·ªán t·∫°i kh√¥ng c√≥ th√¥ng tin chi ti·∫øt v·ªÅ s·∫£n ph·∫©m n√†y, vui l√≤ng th·ª≠ t√¨m ki·∫øm s·∫£n ph·∫©m kh√°c."
#             prompt = query
#
#         # T·∫°o c·∫•u tr√∫c tin nh·∫Øn theo ƒë·ªãnh d·∫°ng chat
#         messages = [
#             {
#                 "role": "system",
#                 "content": system_prompt
#             },
#             {
#                 "role": "user",
#                 "content": prompt
#             }
#         ]
#
#         # √Åp d·ª•ng chat template v√† tokenize
#         inputs = self.tokenizer.apply_chat_template(
#             messages if isinstance(messages[0], dict) else messages[0],
#             add_generation_prompt=True,
#             tokenize=True,
#             return_dict=True,
#             return_tensors="pt"
#         ).to(self.model.device)
#
#         # Sinh c√¢u tr·∫£ l·ªùi
#         with torch.no_grad(), torch.inference_mode():
#             outputs = self.model.generate(
#                 **inputs,
#                 max_new_tokens=max_new_tokens,
#                 temperature=0.7,
#                 top_p=0.9,
#                 do_sample=True
#             )
#
#         # Gi·∫£i m√£ v√† tr·∫£ v·ªÅ k·∫øt qu·∫£
#         decoded_output = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]
#
#         # X·ª≠ l√Ω k·∫øt qu·∫£ ƒë·ªÉ ch·ªâ l·∫•y ph·∫ßn ph·∫£n h·ªìi c·ªßa m√¥ h√¨nh
#         assistant_prefix = "assistant"
#         if assistant_prefix in decoded_output.lower():
#             response = decoded_output.lower().split(assistant_prefix, 1)[1].strip()
#             if response.startswith(": "):
#                 response = response[2:]
#             return response
#
#         return decoded_output


import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer
from threading import Thread
import os
from huggingface_hub import login

class GemmaModel:
    def __init__(self):
        # Kh·ªüi t·∫°o tokenizer
        hf_token = os.environ.get("HF_TOKEN")
        if hf_token:
            login(token=hf_token)
            print("üîê ƒê√£ x√°c th·ª±c v·ªõi Hugging Face")
            
        self.model_id = "google/gemma-3-1b-it"
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)

        # T·∫£i m√¥ h√¨nh kh√¥ng s·ª≠ d·ª•ng l∆∞·ª£ng t·ª≠ h√≥a
        print("ƒêang t·∫£i m√¥ h√¨nh Gemma 3 v√†o CPU...")
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_id,
            device_map="cuda",
            torch_dtype=torch.float32
        ).eval()
        print("ƒê√£ t·∫£i xong m√¥ h√¨nh Gemma 3!")

    def generate_response_stream(self, query, context=None, max_new_tokens=512):
        """
        Sinh ph·∫£n h·ªìi d∆∞·ªõi d·∫°ng streaming - tr·∫£ v·ªÅ t·ª´ng token khi ƒë∆∞·ª£c t·∫°o ra
        """
        # X√¢y d·ª±ng prompt d·ª±a tr√™n ng·ªØ c·∫£nh RAG

        query_lower = query.lower().strip()

        # Ch√†o h·ªèi
        greetings = ['xin ch√†o', 'ch√†o', 'hello', 'hi', 'ch√†o b·∫°n', 'ch√†o anh', 'ch√†o ch·ªã']
        if any(greeting in query_lower for greeting in greetings):
            yield "T√¥i c√≥ th·ªÉ gi√∫p g√¨ b·∫°n?"
            return
            
        if context:
            system_prompt = """
             """
            prompt = f"D·ª±a v√†o th√¥ng tin s·∫£n ph·∫©m sau ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi. Th√¥ng tin s·∫£n ph·∫©m:\n{context}\n\nC√¢u h·ªèi: {query}."
        else:
            system_prompt = "B·∫°n l√† tr·ª£ l√Ω AI tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ s·∫£n ph·∫©m. Hi·ªán t·∫°i kh√¥ng c√≥ th√¥ng tin chi ti·∫øt v·ªÅ s·∫£n ph·∫©m n√†y, vui l√≤ng th·ª≠ t√¨m ki·∫øm s·∫£n ph·∫©m kh√°c."
            prompt = query

        # T·∫°o c·∫•u tr√∫c tin nh·∫Øn theo ƒë·ªãnh d·∫°ng chat
        messages = [
            {
                "role": "system",
                "content": system_prompt
            },
            {
                "role": "user",
                "content": prompt
            }
        ]

        # √Åp d·ª•ng chat template v√† tokenize
        inputs = self.tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            tokenize=True,
            return_dict=True,
            return_tensors="pt"
        ).to(self.model.device)

        # Kh·ªüi t·∫°o streamer ƒë·ªÉ nh·∫≠n token theo th·ªùi gian th·ª±c
        streamer = TextIteratorStreamer(
            self.tokenizer,
            skip_special_tokens=True,
            skip_prompt=True  # B·ªè qua ph·∫ßn prompt ƒë·∫ßu v√†o
        )

        # Thi·∫øt l·∫≠p tham s·ªë generation
        generation_kwargs = {
            **inputs,
            "streamer": streamer,
            "max_new_tokens": max_new_tokens,
            "temperature": 0.7,
            "top_p": 0.9,
            "do_sample": True,
            "pad_token_id": self.tokenizer.eos_token_id
        }

        # Ch·∫°y generation trong thread ri√™ng bi·ªát
        thread = Thread(target=self._generate_with_streamer, args=(generation_kwargs,))
        thread.start()

        # Yield t·ª´ng token khi n√≥ ƒë∆∞·ª£c t·∫°o ra
        try:
            for new_text in streamer:
                if new_text:  # Ch·ªâ yield n·∫øu c√≥ text
                    yield new_text
        finally:
            thread.join()  # ƒê·∫£m b·∫£o thread k·∫øt th√∫c s·∫°ch s·∫Ω

    def _generate_with_streamer(self, generation_kwargs):
        """Helper method ƒë·ªÉ ch·∫°y generation trong thread ri√™ng"""
        with torch.no_grad(), torch.inference_mode():
            self.model.generate(**generation_kwargs)

    # Gi·ªØ l·∫°i method c≈© cho backward compatibility
    def generate_response(self, query, context=None, max_new_tokens=512):
        """
        T·∫°o c√¢u tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß (non-streaming)
        """
        full_response = ""
        for token in self.generate_response_stream(query, context, max_new_tokens):
            full_response += token
        return full_response